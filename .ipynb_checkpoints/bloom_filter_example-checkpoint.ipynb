{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import random\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from bloom_filter import BloomFilter\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Exercise 1: Create Bloom filter for n = 10^7 with 2% false positive rate\n",
    "\n",
    "# Function for parallel checking using the Bloom filter\n",
    "def parallel_check(element):\n",
    "    return bf_standard.check(element)\n",
    "\n",
    "# Parameters for Bloom Filter\n",
    "n = 10**7  # Number of elements to insert\n",
    "m = 8 * 10**7  # Size of bit array in bits (10 MB)\n",
    "k = int(0.693 * m / n)  # Optimal number of hash functions (k ~ (m/n) * ln(2))\n",
    "\n",
    "# Initialize the Bloom Filter\n",
    "bf_standard = BloomFilter(size=m, hash_count=k)\n",
    "\n",
    "# Pre-generate random numbers for insertion and lookups\n",
    "inserted_elements = [str(random.randint(0, 10**12)) for _ in range(n)]\n",
    "lookup_elements = [str(random.randint(0, 10**12)) for _ in range(10**6)]\n",
    "\n",
    "# Insert all elements into the Bloom filter\n",
    "start_time = time.time()\n",
    "for element in inserted_elements:\n",
    "    bf_standard.add(element)\n",
    "insert_time = time.time() - start_time\n",
    "print(f\"Time taken to insert {n} elements: {insert_time:.2f} seconds\")\n",
    "\n",
    "# Save the inserted elements into a separate file\n",
    "with open('inserted_elements.txt', 'w') as f:\n",
    "    for element in inserted_elements:\n",
    "        f.write(f\"{element}\\n\")\n",
    "\n",
    "# Perform 10^6 lookups that are uniformly randomly selected elements from U\n",
    "start_time = time.time()\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    results = list(executor.map(parallel_check, lookup_elements))\n",
    "false_positive_count = sum(results)\n",
    "lookup_time = time.time() - start_time\n",
    "\n",
    "# Calculate and print the results for random lookups\n",
    "print(f\"Time taken for {10**6} random lookups: {lookup_time:.2f} seconds\")\n",
    "false_positive_rate = false_positive_count / 10**6\n",
    "print(f\"Standard Bloom Filter False Positive Rate (Random Lookups): {false_positive_rate:.2%}\")\n",
    "\n",
    "# Perform 10^6 successful lookups by selecting from inserted elements\n",
    "with open('inserted_elements.txt', 'r') as f:\n",
    "    successful_lookup_elements = [line.strip() for line in f.readlines()[:10**6]]\n",
    "\n",
    "start_time = time.time()\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    successful_results = list(executor.map(parallel_check, successful_lookup_elements))\n",
    "successful_lookup_time = time.time() - start_time\n",
    "\n",
    "# Calculate and print the results for successful lookups\n",
    "print(f\"Time taken for {10**6} successful lookups: {successful_lookup_time:.2f} seconds\")\n",
    "\n",
    "# Visualize the false positive rate\n",
    "plt.bar(['Random Lookups', 'Successful Lookups'], [false_positive_rate, 0], color='skyblue')\n",
    "plt.ylabel('False Positive Rate')\n",
    "plt.ylim(0, 0.1)\n",
    "plt.title('Bloom Filter False Positive Rate (Random vs Successful Lookups)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The time taken for 1,000,000 random lookups was 11.35 seconds.\n",
    "The false positive rate for the random lookups was 2.38%, which is close to the expected 2%. This indicates that the Bloom filter configuration is performing as intended, with a false positive rate that aligns with the target value.\n",
    "\n",
    "\n",
    "The time taken for 1,000,000 successful lookups was 10.37 seconds.\n",
    "The successful lookups took less time than the random lookups from the entire universe (U), which may be due to the overhead of checking elements that are not in the filter, as well as potentially higher cache efficiency when working with elements that were inserted.\n",
    "\n",
    "\n",
    "In this case, uniform random lookups took more time (11.35 seconds) compared to successful lookups (10.37 seconds).\n",
    "The difference in lookup times suggests that when querying elements that are present in the Bloom filter, the process may be slightly more efficient compared to lookups that may yield false positives. The false positive rate being close to 2% also confirms that the Bloom filter is well-configured for the given parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Exercise 2: Compare dataset sizes (100x larger and 100x smaller)\n",
    "\n",
    "# Larger dataset (100x)\n",
    "n_large = 10**9\n",
    "m_large = 8 * 10**9\n",
    "k_large = int(0.693 * m_large / n_large)\n",
    "bf_large = BloomFilter(size=m_large, hash_count=k_large)\n",
    "\n",
    "# Insert elements into the larger Bloom filter\n",
    "inserted_large_elements = [str(random.randint(0, 10**12)) for _ in range(n_large)]\n",
    "start_time = time.time()\n",
    "for element in inserted_large_elements:\n",
    "    bf_large.add(element)\n",
    "insert_large_time = time.time() - start_time\n",
    "print(f\"Time taken to insert {n_large} elements into larger Bloom Filter: {insert_large_time:.2f} seconds\")\n",
    "\n",
    "# Smaller dataset (100x)\n",
    "n_small = 10**5\n",
    "m_small = 8 * 10**5\n",
    "k_small = int(0.693 * m_small / n_small)\n",
    "bf_small = BloomFilter(size=m_small, hash_count=k_small)\n",
    "\n",
    "# Insert elements into the smaller Bloom filter\n",
    "inserted_small_elements = [str(random.randint(0, 10**12)) for _ in range(n_small)]\n",
    "start_time = time.time()\n",
    "for element in inserted_small_elements:\n",
    "    bf_small.add(element)\n",
    "insert_small_time = time.time() - start_time\n",
    "print(f\"Time taken to insert {n_small} elements into smaller Bloom Filter: {insert_small_time:.2f} seconds\")\n",
    "\n",
    "# Visualize insertion times for different dataset sizes\n",
    "sizes = ['Standard (10^7)', 'Large (10^9)', 'Small (10^5)']\n",
    "insertion_times = [insert_time, insert_large_time, insert_small_time]\n",
    "plt.bar(sizes, insertion_times, color=['skyblue', 'lightgreen', 'salmon'])\n",
    "plt.ylabel('Insertion Time (seconds)')\n",
    "plt.title('Insertion Time for Different Dataset Sizes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original Dataset\n",
    "\n",
    "- **n (number of elements)**: $10^7$\n",
    "- **m (bit array size)**: $8 \\times 10^7$ bits (or 10 MB)\n",
    "- **k (number of hash functions)**: Approximately 6\n",
    "\n",
    "The false positive rate for this Bloom filter was approximately 2%.\n",
    "\n",
    "## Larger Dataset (100x the original size)\n",
    "\n",
    "- **n (number of elements)**: $10^9$\n",
    "- **m (bit array size)**: $8 \\times 10^9$ bits (or 1 GB)\n",
    "- **k (number of hash functions)**: Approximately 6\n",
    "\n",
    "The size of the filter increased proportionally with the dataset size. To maintain the same false positive rate, the filter size had to increase significantly. Specifically, the bit array size needed to be 100 times larger than the original filter.\n",
    "\n",
    "## Smaller Dataset (1/100th of the original size)\n",
    "\n",
    "- **n (number of elements)**: $10^5$\n",
    "- **m (bit array size)**: $8 \\times 10^5$ bits (or 100 KB)\n",
    "- **k (number of hash functions)**: Approximately 6\n",
    "\n",
    "Similarly, when the dataset size decreased, the filter size also decreased proportionally. The bit array size needed to be 100 times smaller than the original filter to maintain the same false positive rate.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "The size of the Bloom filter ($m$, the bit array size) increases linearly with the number of elements $n$ when keeping the false positive rate $f$ constant.\n",
    "\n",
    "To achieve the same false positive rate, the bit array size must be scaled proportionally to the dataset size. This demonstrates that maintaining a low false positive rate for very large datasets requires significantly more memory, while smaller datasets can achieve the same false positive rate with much less memory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Exercise 3: Implement jurisdictional hashing and measure performance\n",
    "\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import matplotlib.pyplot as plt\n",
    "from bloom_filter import BloomFilter\n",
    "\n",
    "# Timing and comparison logic\n",
    "def time_function(func, *args, **kwargs):\n",
    "    \"\"\"A utility function to measure the time a function takes to execute.\"\"\"\n",
    "    start_time = time.time()\n",
    "    result = func(*args, **kwargs)\n",
    "    end_time = time.time()\n",
    "    return result, end_time - start_time\n",
    "\n",
    "# Initialize the Standard Bloom Filter and the Jurisdictional, Optimized, and Universal Bloom Filters\n",
    "bf_standard = BloomFilter(size=1000000, hash_count=5)\n",
    "bf_jurisdictional = BloomFilter(size=1000000, hash_count=5)\n",
    "bf_optimized = BloomFilter(size=1000000, hash_count=5)\n",
    "bf_universal = BloomFilter(size=1000000, hash_count=5)\n",
    "\n",
    "# Add 100,000 elements to all filters\n",
    "inserted_elements = [f\"item{i}\" for i in range(10**5)]\n",
    "lookup_elements = [f\"item{i}\" for i in range(10**6)]  # 1 million elements for false positive tests\n",
    "\n",
    "# Add elements to the Standard Bloom Filter\n",
    "for element in inserted_elements:\n",
    "    bf_standard.add(element)\n",
    "\n",
    "# Add elements to the Jurisdictional Bloom Filter\n",
    "for element in inserted_elements:\n",
    "    bf_jurisdictional.add(element, jurisdiction=True)\n",
    "\n",
    "# Add elements to the Optimized Bloom Filter\n",
    "for element in inserted_elements:\n",
    "    bf_optimized.add(element, optimized=True)\n",
    "\n",
    "# Add elements to the Universal Bloom Filter\n",
    "for element in inserted_elements:\n",
    "    bf_universal.add(element, universal=True)\n",
    "\n",
    "# Measure false positives and time for Standard, Jurisdictional, Optimized, and Universal methods\n",
    "# Standard Bloom Filter\n",
    "start_time = time.time()\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    results_standard = list(executor.map(lambda x: bf_standard.check(x), lookup_elements))\n",
    "false_positive_count_standard = sum(results_standard)\n",
    "lookup_time_standard = time.time() - start_time\n",
    "\n",
    "# Jurisdictional Bloom Filter\n",
    "start_time = time.time()\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    results_jurisdictional = list(executor.map(lambda x: bf_jurisdictional.check(x, jurisdiction=True), lookup_elements))\n",
    "false_positive_count_jurisdictional = sum(results_jurisdictional)\n",
    "lookup_time_jurisdictional = time.time() - start_time\n",
    "\n",
    "# Optimized Bloom Filter (Kirsch-Mitzenmacher Optimization)\n",
    "start_time = time.time()\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    results_optimized = list(executor.map(lambda x: bf_optimized.check(x, optimized=True), lookup_elements))\n",
    "false_positive_count_optimized = sum(results_optimized)\n",
    "lookup_time_optimized = time.time() - start_time\n",
    "\n",
    "# Universal Bloom Filter\n",
    "start_time = time.time()\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    results_universal = list(executor.map(lambda x: bf_universal.check(x, universal=True), lookup_elements))\n",
    "false_positive_count_universal = sum(results_universal)\n",
    "lookup_time_universal = time.time() - start_time\n",
    "\n",
    "# Calculate false positive rates\n",
    "false_positive_rate_standard = false_positive_count_standard / 10**6\n",
    "false_positive_rate_jurisdictional = false_positive_count_jurisdictional / 10**6\n",
    "false_positive_rate_optimized = false_positive_count_optimized / 10**6\n",
    "false_positive_rate_universal = false_positive_count_universal / 10**6\n",
    "\n",
    "# Print results\n",
    "print(f\"Standard Bloom Filter False Positive Rate: {false_positive_rate_standard:.2%}\")\n",
    "print(f\"Time taken for standard lookups: {lookup_time_standard:.2f} seconds\")\n",
    "\n",
    "print(f\"Jurisdictional Bloom Filter False Positive Rate: {false_positive_rate_jurisdictional:.2%}\")\n",
    "print(f\"Time taken for jurisdictional lookups: {lookup_time_jurisdictional:.2f} seconds\")\n",
    "\n",
    "print(f\"Optimized Bloom Filter False Positive Rate: {false_positive_rate_optimized:.2%}\")\n",
    "print(f\"Time taken for optimized lookups: {lookup_time_optimized:.2f} seconds\")\n",
    "\n",
    "print(f\"Universal Bloom Filter False Positive Rate: {false_positive_rate_universal:.2%}\")\n",
    "print(f\"Time taken for universal lookups: {lookup_time_universal:.2f} seconds\")\n",
    "\n",
    "# Visualize false positive rates for different methods\n",
    "methods = ['Standard', 'Jurisdictional', 'Optimized', 'Universal']\n",
    "rates = [false_positive_rate_standard, false_positive_rate_jurisdictional, false_positive_rate_optimized, false_positive_rate_universal]\n",
    "plt.bar(methods, rates, color=['skyblue', 'lightcoral', 'lightgreen', 'lightyellow'])\n",
    "plt.ylabel('False Positive Rate')\n",
    "plt.ylim(0, 0.1)\n",
    "plt.title('False Positive Rate Comparison')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. jurisdictional Bloom Filter:\n",
    "\n",
    "False Positive Rate: 10.87%\n",
    "Time Taken for Lookups: 10.51 seconds\n",
    "\n",
    "\n",
    "Analysis: Jurisdictional hashing, where the bit array is divided into chunks, showed the highest false positive rate. One possible explanation for this high rate is that by restricting the hash values to specific chunks, there may be more overlap within each chunk. This reduces the overall space available for independent hash values, increasing the chances of false positives.\n",
    "\n",
    "In terms of time, jurisdictional hashing was slower than optimized hashing but faster than universal hashing. This method adds complexity because of the division into chunks, which likely increased the overhead during lookups.\n",
    "\n",
    "Conclusion: Jurisdictional hashing may not offer significant improvements in terms of false positives and performance and may not be suitable for cases where minimizing false positives is critical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2. Optimized Bloom Filter (Kirsch-Mitzenmacher Optimization):\n",
    "\n",
    "\n",
    "False Positive Rate: 10.85%\n",
    "Time Taken for Lookups: 9.13 seconds\n",
    "Analysis: The optimized Bloom filter using the Kirsch-Mitzenmacher technique yielded a slightly lower false positive rate than jurisdictional hashing (10.85% vs. 10.87%), which is almost negligible in difference. However, the major advantage of the Kirsch-Mitzenmacher optimization is in the lookup time, where it was the fastest among the three methods, taking only 9.13 seconds. This reduction in time is due to the use of just two hash functions to derive the other hash values, reducing the number of computations required during lookups.\n",
    "\n",
    "Conclusion: The Kirsch-Mitzenmacher optimization offers a slight improvement in performance while keeping the false positive rate similar to the jurisdictional approach. It is more efficient for scenarios where speed is a priority, but does not significantly reduce false positives.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "3. Universal Hashing:\n",
    "\n",
    "\n",
    "False Positive Rate: 0.95%\n",
    "Time Taken for Lookups: 12.32 seconds\n",
    "Analysis: Universal hashing exhibited a dramatic improvement in the false positive rate, achieving only 0.95%, which is far lower than both jurisdictional and optimized methods (around 10x better). This shows that universal hashing provides much better distribution of hash values, which reduces the chance of false positives. However, this comes with a trade-off: lookup time was the highest among the methods, taking 12.32 seconds. This increase in time is likely due to the added complexity of generating hash values using random primes and seeds, which is computationally more expensive.\n",
    "\n",
    "Conclusion: Universal hashing offers a significant reduction in false positives, making it an excellent choice for applications where accuracy is more important than speed. However, the slower performance may be a concern in scenarios where time efficiency is critical."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
